Rank 1's generation took 3.4457664489746094 seconds..
[rank0]:[E428 15:42:53.371860804 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2135238, OpType=_ALLGATHER_BASE, NumelIn=3735552, NumelOut=11206656, Timeout(ms)=1800000) ra
n for 1800006 milliseconds before timing out.
[rank0]:[E428 15:42:53.427706275 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 2135238 PG status: last enqueued work: 2135247, last completed work: 213
5237
[rank0]:[E428 15:42:53.427745855 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-
zero value.
[rank0]:[E428 15:42:53.427756785 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete
 data.
[rank0]:[E428 15:42:53.427766045 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E428 15:42:53.435101277 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(Se
qNum=2135238, OpType=_ALLGATHER_BASE, NumelIn=3735552, NumelOut=11206656, Timeout(ms)=1800000) ran for 1800006 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f01709a61b6 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f0171cefc74 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/
lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f0171cf17d0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f0171cf26ed in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f01bad755c0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x8609 (0x7f026f520609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f026f2eb133 in /lib/x86_64-linux-gnu/libc.so.6) 


[rank2]:[E428 15:42:53.450193339 ProcessGroupNCCL.cpp:629] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2135239, OpType=_ALLGATHER_BASE, NumelIn=512, NumelOut=1536, Timeout(ms)=1800000) ran for 18
00010 milliseconds before timing out.
[rank2]:[E428 15:42:53.451089187 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 2135239 PG status: last enqueued work: 2135240, last completed work: 213
5238
[rank2]:[E428 15:42:53.451114617 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-
zero value.
[rank2]:[E428 15:42:53.451123727 ProcessGroupNCCL.cpp:681] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete
 data.
[rank2]:[E428 15:42:53.451132147 ProcessGroupNCCL.cpp:695] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E428 15:42:53.451311036 ProcessGroupNCCL.cpp:629] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2135239, OpType=_ALLGATHER_BASE, NumelIn=512, NumelOut=1536, Timeout(ms)=1800000) ran for 18
00011 milliseconds before timing out.
[rank1]:[E428 15:42:53.452259074 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 2135239 PG status: last enqueued work: 2135240, last completed work: 213
5238
[rank1]:[E428 15:42:53.452282514 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E428 15:42:53.452289774 ProcessGroupNCCL.cpp:681] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E428 15:42:53.452295394 ProcessGroupNCCL.cpp:695] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E428 15:42:53.453689981 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2135239, OpType=_ALLGATHER_BASE, NumelIn=512, NumelOut=1536, Timeout(ms)=1800000) ran for 1800010 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6940d4e1b6 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f6942097c74 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f69420997d0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f694209a6ed in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f698b11d5c0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x8609 (0x7f6a3f8c8609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f6a3f693133 in /lib/x86_64-linux-gnu/libc.so.6) 


[rank1]:[E428 15:42:53.453967990 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2135239, OpType=_ALLGATHER_BASE, NumelIn=512, NumelOut=1536, Timeout(ms)=1800000) ran for 1800011 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f5f674841b6 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7f5f687cdc74 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7f5f687cf7d0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f5f687d06ed in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f5fb18535c0 in /home/907308160/code/nairr/.conda/lib/python3.12/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x8609 (0x7f6065ffe609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #6: clone + 0x43 (0x7f6065dc9133 in /lib/x86_64-linux-gnu/libc.so.6) 

[2025-04-28 15:42:54,886] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 153072
[2025-04-28 15:42:54,907] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 153073
[2025-04-28 15:42:54,907] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 153074
[2025-04-28 15:42:54,919] [ERROR] [launch.py:325:sigkill_handler] ['/home/907308160/code/nairr/.conda/bin/python', '-u', 'main_deepspeed.py', '--local_rank=2', '--fresh_weights', '--num_gpus', '3', '-ep', '4', '-ss', '1.00', '--experiment_name', 'vatex_ds', '--train_batch_size=384', '-nahe', '12', '-nhle', '12', '-nld', '12', '-nhd', '12', '--do_train', '--do_val', '--do_test'] exits with return code = -6
Command exited with non-zero status 250
352674.14user 7477.11system 18:42:32elapsed 534%CPU (0avgtext+0avgdata 2666600maxresident)k
2107158712inputs+25701712outputs (3160major+1419490691minor)pagefaults 0swaps
(/home/907308160/code/nairr/.conda) 907308160@srv-hh-306-133:~/code/nairr$ 
